{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### This project is intented to demonstrate the power of supply chain analysis. Using a database sourced from the Open Food Database of a popular dish known as Avocado Toast, data was filtered by specific criteria returning some useful information to provide greater insight into a global supply chain."
      ],
      "metadata": {
        "id": "V4r1Y1QtBVlW"
      },
      "id": "V4r1Y1QtBVlW"
    },
    {
      "source": [
        "def read_and_filter_data(filename, relevant_categories):\n",
        "    #read csv file specifically tab delimited\n",
        "    dataframe= pd.read_csv(filename, sep='\\t')\n",
        "\n",
        "    #read text file and use splitlines method to assign relevant categories\n",
        "    with open(relevant_categories,\"r\") as file:\n",
        "        relevant_categories = file.read().splitlines()\n",
        "        file.close()\n",
        "\n",
        "    #Subset dataframe to be more manageable using more precise categories\n",
        "    relevant_labels = ['code', 'lc', 'product_name_en', 'quantity', 'serving_size',\n",
        "                           'packaging_tags', 'brands', 'brands_tags', 'categories_tags',\n",
        "                           'labels_tags', 'countries', 'countries_tags',\n",
        "                       'origins','origins_tags']\n",
        "    dataframe = dataframe[relevant_labels]\n",
        "\n",
        "    #category tags need to be split into a list and then null, NaN values dropped\n",
        "    dataframe['categories_tags'] = dataframe['categories_tags'].str.split(',')\n",
        "    dataframe = dataframe.dropna(subset='categories_tags')\n",
        "\n",
        "    #Iterate over relevant value in rows, any() checks if a match to external file\n",
        "    dataframe = dataframe[dataframe['categories_tags'].apply(lambda x: any([i for i in x if\n",
        "                                                                            i in\n",
        "                                                                            relevant_categories]))]\n",
        "    #Subset by country and assign filtered data to new dataframe\n",
        "    dataframe_uk = dataframe[(dataframe['countries'] == 'United Kingdom')]\n",
        "\n",
        "    #This counts the value of the origin tags that match selected country above\n",
        "    dataframe_uk['origins_tags'].value_counts()\n",
        "\n",
        "    #Subset top value\n",
        "    ingredient_origin = (dataframe_uk['origins_tags'].value_counts().index[0])\n",
        "\n",
        "    #Format accordingly by removing prefix\n",
        "    ingredient_origin = ingredient_origin.lstrip('en:')\n",
        "\n",
        "    #Format accordingly by removing dashes\n",
        "    ingredient_origin = ingredient_origin.replace('-', ' ')\n",
        "\n",
        "    return ingredient_origin\n",
        "\n"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 51,
        "lastExecutedAt": 1740959866227,
        "lastExecutedByKernel": "142eff01-6efb-412a-93d9-5953ba233bd1",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "def read_and_filter_data(filename, relevant_categories):\n    #read csv file specifically tab delimited\n    dataframe= pd.read_csv(filename, sep='\\t')\n    \n    #read text file and use splitlines method to assign relevant categories\n    with open(relevant_categories,\"r\") as file:\n        relevant_categories = file.read().splitlines()\n        file.close()\n        \n    #Subset dataframe to be more manageable using more precise categories\n    relevant_labels = ['code', 'lc', 'product_name_en', 'quantity', 'serving_size',\n                           'packaging_tags', 'brands', 'brands_tags', 'categories_tags',\n                           'labels_tags', 'countries', 'countries_tags',\n                       'origins','origins_tags']\n    dataframe = dataframe[relevant_labels]\n    \n    #category tags need to be split into a list and then null, NaN values dropped\n    dataframe['categories_tags'] = dataframe['categories_tags'].str.split(',')\n    dataframe = dataframe.dropna(subset='categories_tags')\n    \n    #Iterate over relevant value in rows, any() checks if a match to external file\n    dataframe = dataframe[dataframe['categories_tags'].apply(lambda x: any([i for i in x if\n                                                                            i in\n                                                                            relevant_categories]))]\n    #Subset by country and assign filtered data to new dataframe\n    dataframe_uk = dataframe[(dataframe['countries'] == 'United Kingdom')]\n    \n    #This counts the value of the origin tags that match selected country above\n    dataframe_uk['origins_tags'].value_counts()\n    \n    #Subset top value\n    ingredient_origin = (dataframe_uk['origins_tags'].value_counts().index[0])\n    \n    #Format accordingly by removing prefix\n    ingredient_origin = ingredient_origin.lstrip('en:')\n    \n    #Format accordingly by removing dashes\n    ingredient_origin = ingredient_origin.replace('-', ' ')\n    \n    return ingredient_origin\n    \n    ",
        "id": "f745fdb8-404b-41ac-958c-9e318ae9a908"
      },
      "cell_type": "code",
      "id": "f745fdb8-404b-41ac-958c-9e318ae9a908",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "top_avocado_origin = read_and_filter_data('data/avocado.csv','data/relevant_avocado_categories.txt')\n",
        "print(top_avocado_origin)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 61,
        "lastExecutedAt": 1740959871505,
        "lastExecutedByKernel": "142eff01-6efb-412a-93d9-5953ba233bd1",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "top_avocado_origin = read_and_filter_data('data/avocado.csv','data/relevant_avocado_categories.txt')\nprint(top_avocado_origin)",
        "outputsMetadata": {
          "0": {
            "height": 38,
            "type": "stream"
          }
        },
        "id": "900cfdb6-7dd6-4570-a6cf-643037de41b5",
        "outputId": "21e2c288-4174-41a5-81b1-9a19bd10ccbc"
      },
      "cell_type": "code",
      "id": "900cfdb6-7dd6-4570-a6cf-643037de41b5",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "peru\n"
        }
      ],
      "execution_count": null
    },
    {
      "source": [
        "top_olive_oil_origin = read_and_filter_data('data/olive_oil.csv','data/relevant_olive_oil_categories.txt')\n",
        "print(top_olive_oil_origin)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 372,
        "lastExecutedAt": 1740959874785,
        "lastExecutedByKernel": "142eff01-6efb-412a-93d9-5953ba233bd1",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "top_olive_oil_origin = read_and_filter_data('data/olive_oil.csv','data/relevant_olive_oil_categories.txt')\nprint(top_olive_oil_origin)",
        "outputsMetadata": {
          "0": {
            "height": 38,
            "type": "stream"
          }
        },
        "id": "492cc941-b218-4afc-9d10-230a38890368",
        "outputId": "f78bf928-3187-4811-c519-bde2c69fa9f0"
      },
      "cell_type": "code",
      "id": "492cc941-b218-4afc-9d10-230a38890368",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "greece\n"
        }
      ],
      "execution_count": null
    },
    {
      "source": [
        "top_sourdough_origin = read_and_filter_data('data/sourdough.csv','data/relevant_sourdough_categories.txt')\n",
        "print(top_sourdough_origin)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 56,
        "lastExecutedAt": 1740959878769,
        "lastExecutedByKernel": "142eff01-6efb-412a-93d9-5953ba233bd1",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "top_sourdough_origin = read_and_filter_data('data/sourdough.csv','data/relevant_sourdough_categories.txt')\nprint(top_sourdough_origin)",
        "outputsMetadata": {
          "0": {
            "height": 38,
            "type": "stream"
          }
        },
        "id": "9ed6f99b-4250-4648-847e-5bbcb9f4a835",
        "outputId": "e2a09d99-770a-4eb8-f045-c67b5ed66826"
      },
      "cell_type": "code",
      "id": "9ed6f99b-4250-4648-847e-5bbcb9f4a835",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "united kingdom\n"
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}